{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 3: document models\n",
    "_[Original](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/cw/coursework3.pdf) by Carl Rasmussen and Manon Kok for [CUED course 4f13](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/). This version adapted by Damon Wischik._\n",
    "\n",
    "This coursework involves aggregating, summarizing, and joining datasets. This may be done with straight Python, or with MATLAB-style manipulations using `numpy`, or with `pandas` dataframes. If you anticipate future work in machine learning and data science then you should learn to use `pandas` dataframes, and you may find it helpful to follow the walkthrough in [Section 3](https://notebooks.azure.com/djw1005/libraries/cl-scicomp/html/3.%20Working%20with%20data.ipynb) of IA _Scientific Computing_. If you prefer not to use dataframes, and you have questions about how they are being used in the code snippets below, ask your classmates or Dr Wischik.\n",
    "\n",
    "**What to submit.**\n",
    "Your answers should contain an explanation of what you do, and\n",
    "2&ndash;4 central commands to achieve it. Complete listings are\n",
    "unnecessary. The focus of your answer should be\n",
    "_interpretation:_ explain what the numerical values and graphs\n",
    "you produce _mean,_ and why they are as they are.  The text of\n",
    "your answer to each question should be no more than a paragraph or\n",
    "two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import pandas\n",
    "import requests, io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "The data is provided as `http://mlg.eng.cam.ac.uk/teaching/4f13/1112/cw/kos_doc_data.mat`. It contains two matrices $A$ and $B$ for training and testing respectively, both matrices with 3 columns: document ID, word ID, and word count. The words themselves are the vector $V$, where e.g. `V[840]='bush'`. The following snippet reads in the data, and converts $A$ and $B$ to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://mlg.eng.cam.ac.uk/teaching/4f13/1112/cw/kos_doc_data.mat')\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    V = np.array([i[0] for i in data['V'].squeeze()])\n",
    "    A,B = [pandas.DataFrame({'doc_id': M[:,0]-1, 'word_id': M[:,1]-1, 'count': M[:,2]}, \n",
    "                            columns=['doc_id','word_id','count']) \n",
    "           for M in (data['A'],data['B'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (a): simple categorical model\n",
    "\n",
    "Suppose we model words in a document as independent samples from a categorical distribution with parameter $\\beta$, where $\\beta_v$ is the probability of word $v\\in V$. Using $A$ as the training set, find the maximum likelihood estimator $\\hat{\\beta}$, and plot the 20 most-probable words in a histogram. What is the log probability of the test document `doc_id=2527`, given $\\hat{\\beta}$? Briefly interpret your answer.\n",
    "\n",
    "Note: you can plot a histogram with\n",
    "```\n",
    "with plt.rc_context({'figure.figsize': (5,8)}):            # set plot size\n",
    "    plt.barh(np.arange(20), top_20_probs, align='center')  # draw bars\n",
    "plt.yticks(np.arange(20), top_20_words)                    # label the y axis\n",
    "plt.xlabel(r'$\\hat{\\beta}$')                               # label the x axis\n",
    "plt.gca().invert_yaxis()                                   # optionally, flip the y-axis\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (b): Bayesian inference\n",
    "\n",
    "For the categorical model in part (a), use Bayesian inference to find the posterior distribution of $\\beta$ given the training set $A$, using a symmetric Dirichlet distribution with concentration parameter $\\alpha=0.1$ as prior. Let $\\tilde{\\beta}_v$ be the posterior predictive probability of word $v\\in V$, i.e. the posterior probability that a newly chosen word is $v$. Derive an expression for $\\tilde{\\beta}_v$, and compare it to $\\hat{\\beta}_v$. Explain the implications, both for common and for rare words.\n",
    "\n",
    "Hint: $\\Gamma(z+1)=z\\,\\Gamma(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (c): interpretation\n",
    "\n",
    "In information theory, the _self-information_ of a document $w$ is defined as $i(w) = -\\log_2 p(w)$, where $p(\\cdot)$ is the probability mass function for the document generating model that you have fitted. The self-information can be interpreted as the number of bits needed to encode or transmit $w$. The number of bits needed per word is thus $i(w)/n$. In text modelling, it is more common to use the terms _perplexity_ for $2^{i(w)}$, and _per-word perplexity_ for $2^{i(w)/n}$. Loosely speaking, if the per-word perplexity is $g$ then the uncertainty in the next word is the same as the uncertainty in a $g$-sided die.\n",
    "\n",
    "For the trained Bayesian model from part (b), what is the per-word perplexity of the test document `doc_id=2000`? Plot a histogram showing the distribution of per-word perplexity over all the test documents (using [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html?highlight=matplotlib%20pyplot%20hist#matplotlib.pyplot.hist)). Pick out two documents, one with high per-word perplexity and one with low per-word perplexity, show their contents, and interpret the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (d): Gibbs sampler for the mixture-of-multinomials model\n",
    "\n",
    "The Bayesian mixture-of-multinomials model can be described by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['critic' 'indymedia' 'susan' 'citizenship' 'cycles']\n",
      "['cool' 'celebrity']\n",
      "['jennings' 'quarter' 'token' 'governance']\n"
     ]
    }
   ],
   "source": [
    "def bmm_generate(doc_length, V, α, γ, K):\n",
    "    # doc_length = [num words in doc1, num words in doc2, ...]\n",
    "    θ = np.random.dirichlet(α * np.ones(K))              # prob dist over document classes {1,...,K}\n",
    "    β = np.random.dirichlet(γ * np.ones(len(V)), size=K) # for each doc class, a prob dist over words\n",
    "    z = np.random.choice(K, p=θ, size=len(doc_length))   # doc class of each document\n",
    "    return [np.random.choice(V, p=β[zd], size=nd) for zd,nd in zip(z, doc_length)]\n",
    "\n",
    "for doc in bmm_generate(doc_length=[5,2,4], V=V, α=10, γ=.1, K=20):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a collapsed Gibbs sampler. Complete the line that defines `logp`. In each sweep, the Gibbs sampler produces a sample of document classes, and this sample induces a posterior predictive distribution for the probability of each class. Plot how this distribution evolves as a function of the number of Gibbs sweeps. How many iterations does it take to converge?\n",
    "```\n",
    "def bmm_gibbs(doc_label, word_id, count, W, α, γ, K):\n",
    "    # doc_labels = distinct values of doc_label\n",
    "    # doc_index = a list as long as doc_label\n",
    "    #             such that doc_labels[doc_index[j]] = doc_label[j]\n",
    "    doc_labels, doc_index = np.unique(doc_label, return_inverse=True)\n",
    "\n",
    "    # z[i] = class of document i, where i enumerates the distinct doc_labels\n",
    "    # doc_count[k] = number of documents of class k\n",
    "    z = np.random.choice(K, len(doc_labels))\n",
    "    doc_count = np.zeros(K, dtype=int)\n",
    "    for k in z: doc_count[k] += 1\n",
    "\n",
    "    # occurrences[k,w] = number of occurrences of word_id w in documents of class k\n",
    "    # word_count[k] = total number of words in documents of class k\n",
    "    x = pandas.DataFrame({'doc_class': z[doc_index], 'word_id': word_id, 'count': count}) \\\n",
    "        .groupby(['doc_class', 'word_id']) \\\n",
    "        ['count'].apply(sum) \\\n",
    "        .unstack(fill_value=0)\n",
    "    occurrences = np.zeros((K, W))\n",
    "    occurrences[x.index.values.reshape((-1,1)), x.columns.values] = x\n",
    "    word_count = np.sum(occurrences, axis=1)\n",
    "    \n",
    "    while True:\n",
    "        for i in range(len(doc_labels)):\n",
    "\n",
    "            # get the words,counts for document i\n",
    "            # and remove this document from the counts\n",
    "            w,c = word_id[doc_index==i].values, count[doc_index==i].values\n",
    "            occurrences[z[i], w] -= c\n",
    "            word_count[z[i]] -= sum(c)\n",
    "            doc_count[z[i]] -= 1\n",
    "\n",
    "            # Find the log probability that this document belongs to class k, marginalized over θ and β\n",
    "            logp = [... for k in range(K)]\n",
    "            p = np.exp(logp - np.max(logp))\n",
    "            p = p/sum(p)\n",
    "\n",
    "            # Assign this document to a new class, chosen randomly, and add back the counts\n",
    "            k = np.random.choice(K, p=p)\n",
    "            z[i] = k\n",
    "            occurrences[k, w] += c\n",
    "            word_count[k] += sum(c)\n",
    "            doc_count[k] += 1\n",
    "        \n",
    "        yield np.copy(z)\n",
    "```\n",
    "The Gibbs sampler may be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = bmm_gibbs(A['doc_id'], A['word_id'], A['count'], W=len(V), α=10, γ=.1, K=20)\n",
    "NUM_ITERATIONS = 20\n",
    "res = np.stack([next(g) for _ in range(NUM_ITERATIONS)])\n",
    "# this produces a matrix with one row per iteration and a column for each unique doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (e): interpretation\n",
    "\n",
    "Let $\\alpha=10$, $\\gamma=0.1$, $K=20$. Run the Gibbs sampler until it converges, and find the posterior predictive probabilities for topics, and for words within each topic. \n",
    "For each the 8 most popular topics, print the probability of the topic and the 8 most probable words and their probabilities.\n",
    "Display probabilities in _shannons_, i.e. display a probability $p$ as $-\\log_2 p$. An increase of 1 shannon corresponds to a 50% decrease in probability.\n",
    "\n",
    "Rerun with different random seeds. Do you think this method has succeeded in identifying topics?\n",
    "\n",
    "**Optional.** There are some words that are very common across all topics. How might we pick out the _distinctive_ words for each topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (f): evaluation\n",
    "\n",
    "**Optional.** Give a formula for per-word perplexity for the mixture model, in terms of the posterior predictive probabilities for topics and words.\n",
    "\n",
    "**Optional.** Plot a histogram showing the distribution of per-word perplexity over all the test documents for the model in part (e). Also plot the histogram obtained from $K=8$, and the histogram from the plain multinomial model in part (c). Which model do you prefer, and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
